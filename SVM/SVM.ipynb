{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0edf672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import string as s\n",
    "import sys\n",
    "\n",
    "import json\n",
    "from random import randrange\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC, SVC, NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6956045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_random(dataset, data_size,split=0.7):\n",
    "    train_size_counter = 0\n",
    "    indices = []\n",
    "    train = pd.DataFrame()\n",
    "    train_size = split * data_size\n",
    "    dataframe_copy = dataset\n",
    "    while train_size_counter < train_size:\n",
    "        train_size_counter = train_size_counter + 1\n",
    "        index = randrange(data_size)\n",
    "        while(check_if_element_in_list(index, indices)):\n",
    "            index = randrange(data_size)\n",
    "                 \n",
    "        indices.append(index)\n",
    "    \n",
    "    for i in range(len(indices)+1):\n",
    "        train = train.append(dataframe_copy.iloc[[i]])\n",
    "        \n",
    "    dataframe_copy = dataframe_copy.drop(labels = indices, axis=0)\n",
    "    dataframe_copy = dataframe_copy.reset_index(drop=True)\n",
    "        \n",
    "    return train, dataframe_copy\n",
    "\n",
    "def check_if_element_in_list(x, lista):\n",
    "    if x in lista:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e158b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(object):\n",
    "    \n",
    "    def __init__(self, train_set, test_set):\n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "\n",
    "    def filter_data_frame(self, data_frame, function):\n",
    "        return data_frame.apply(function)\n",
    "\n",
    "    def tokenize(self):\n",
    "        def split_into_words(input_text):\n",
    "            words = input_text.split()\n",
    "            return words\n",
    "        self.train_set = self.filter_data_frame(self.train_set, split_into_words)\n",
    "        self.test_set = self.filter_data_frame(self.test_set, split_into_words)\n",
    "\n",
    "\n",
    "    def transform_to_Z_case(self, z=\"LOWERCASE\"):\n",
    "        if z == \"LOWERCASE\":\n",
    "            def to_lower_case(words):\n",
    "                transformed_list = [i.lower() for i in words]\n",
    "                return transformed_list\n",
    "            self.train_set = self.filter_data_frame(self.train_set, to_lower_case)\n",
    "            self.test_set = self.filter_data_frame(self.test_set, to_lower_case)\n",
    "        else:\n",
    "            def to_upper_case(words):\n",
    "                transformed_list = [i.upper() for i in words]\n",
    "                return transformed_list\n",
    "            self.train_set = self.filter_data_frame(self.train_set, to_upper_case)\n",
    "            self.test_set = self.filter_data_frame(self.test_set, to_upper_case)\n",
    "\n",
    "    def remove_punctuation_marks(self):\n",
    "        def remove_punctuations(words):\n",
    "            transformed_list = []\n",
    "            for i in words:\n",
    "                for j in s.punctuation:\n",
    "                    i = i.replace(j, '')\n",
    "                transformed_list.append(i)\n",
    "            return transformed_list\n",
    "        self.train_set = self.filter_data_frame(self.train_set, remove_punctuations)\n",
    "        self.test_set = self.filter_data_frame(self.test_set, remove_punctuations)\n",
    "\n",
    "    def trim_text(self):\n",
    "        def trim_spaces_along_word(words):\n",
    "            transformed_list = [i.strip() for i in words]\n",
    "            return transformed_list\n",
    "        self.train_set = self.filter_data_frame(self.train_set, trim_spaces_along_word)\n",
    "        self.test_set = self.filter_data_frame(self.test_set, trim_spaces_along_word)\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        def remove_stop_words(input_text):\n",
    "            stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                         \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
    "                         'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',\n",
    "                         'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is',\n",
    "                         'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
    "                         'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at',\n",
    "                         'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\n",
    "                         'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "                         'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both',\n",
    "                         'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same',\n",
    "                         'so', 'than', 'too', 'very', 's', 't', 'can','cant', 'will', 'just', 'don', \"don't\", \"should\",\n",
    "                         \"should've\",'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "                         'didn',\"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn',\n",
    "                         \"isn't\",'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\",\n",
    "                         'shouldn',\"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\",\n",
    "                         'whos','whose','whats', 'whatre', 'wheres', 'whered','whys']\n",
    "            transformed_list = [i for i in input_text if i not in stopwords]\n",
    "\n",
    "            return transformed_list\n",
    "        self.train_set = self.filter_data_frame(self.train_set, remove_stop_words)\n",
    "        self.test_set = self.filter_data_frame(self.test_set, remove_stop_words)\n",
    "\n",
    "\n",
    "    def detokenize(self, dataset):\n",
    "        return dataset.apply(lambda x: ''.join(i + ' ' for i in x))\n",
    "\n",
    "\n",
    "    def pipe(self):\n",
    "        self.tokenize()\n",
    "        self.transform_to_Z_case() \n",
    "#         self.remove_punctuation_marks()\n",
    "#         self.trim_text()\n",
    "#         self.remove_stopwords()\n",
    "        \n",
    "    def count_words(self):\n",
    "        #Da se vidi koje reci se najvise upotrebljuju\n",
    "        dictionary = {}\n",
    "\n",
    "        for element in self.train_set:\n",
    "            for token in element:\n",
    "                if token in dictionary:\n",
    "                    dictionary[token] += 1\n",
    "                else:\n",
    "                    dictionary[token] = 1\n",
    "\n",
    "        dictionary = sorted(dictionary.items(), key=lambda x: x[1], reverse=True)         \n",
    "        return dictionary\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3185234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9837037037037037\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.read_json('train.json')\n",
    "train_data, test_data = train_test_split_random(all_data, all_data.shape[0], split=0.7 )\n",
    "X_train, Y_train, X_test, Y_test = train_data.text, train_data.clickbait, test_data.text, test_data.clickbait\n",
    "\n",
    "processed_text = Transformer(X_train, X_test)\n",
    "processed_text.pipe()\n",
    "    \n",
    "X_train, X_test = processed_text.train_set, processed_text.test_set\n",
    "X_train = processed_text.detokenize(X_train)\n",
    "X_test = processed_text.detokenize(X_test)\n",
    "    \n",
    "#tfid vectorizer\n",
    "tf_id_vectorizer = TfidfVectorizer(sublinear_tf=True, smooth_idf=False)\n",
    "train_X =  tf_id_vectorizer.fit_transform(X_train).toarray()\n",
    "test_X =  tf_id_vectorizer.transform(X_test).toarray()\n",
    "    \n",
    "linear_SVM = LinearSVC()  #C=0.325, fit_intercept=False\n",
    "linear_SVM.fit(train_X, Y_train)\n",
    "y_pred = linear_SVM.predict(test_X)\n",
    "\n",
    "\n",
    "# svc = SVC(gamma='auto')\n",
    "# svc.fit(train_X, Y_train)\n",
    "# y_pred = svc.predict(test_X)\n",
    "\n",
    "# nu = NuSVC(gamma='scale')\n",
    "# nu.fit(train_X, Y_train)\n",
    "# y_pred = nu.predict(test_X)\n",
    "\n",
    "print(accuracy_score(Y_test, y_pred))\n",
    "#print(f1_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b5c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if(len(sys.argv) != 3):\n",
    "#     print(\"Mora imati dva argumenta 'train.csv' 'test.csv'\")\n",
    "#     exit()\n",
    "# else:\n",
    "#     train_csv = sys.argv[1]\n",
    "#     test_csv = sys.argv[2]  \n",
    "    \n",
    "#     train_data = pd.read_json(train_csv)\n",
    "#     test_data = pd.read_json(test_csv)\n",
    "    \n",
    "#     X_train, Y_train, X_test, Y_test = train_data.text, train_data.clickbait, test_data.text, test_data.clickbait\n",
    "    \n",
    "#     processed_text = Transformer(X_train, X_test)\n",
    "#     processed_text.pipe()\n",
    "\n",
    "#     X_train, X_test = processed_text.train_set, processed_text.test_set\n",
    "#     X_train = processed_text.detokenize(X_train)\n",
    "#     X_test = processed_text.detokenize(X_test)\n",
    "\n",
    "#     #tfid vectorizer\n",
    "#     tf_id_vectorizer = TfidfVectorizer(sublinear_tf=True, smooth_idf=False)\n",
    "#     train_X =  tf_id_vectorizer.fit_transform(X_train).toarray()\n",
    "#     test_X =  tf_id_vectorizer.transform(X_test).toarray()\n",
    "\n",
    "#     linear_SVM = LinearSVC()  #C=0.325, fit_intercept=False\n",
    "#     linear_SVM.fit(train_X, Y_train)\n",
    "#     y_pred = linear_SVM.predict(test_X)\n",
    "\n",
    "\n",
    "#     # svc = SVC(gamma='auto')\n",
    "#     # svc.fit(train_X, Y_train)\n",
    "#     # y_pred = svc.predict(test_X)\n",
    "\n",
    "#     # nu = NuSVC(gamma='scale')\n",
    "#     # nu.fit(train_X, Y_train)\n",
    "#     # y_pred = nu.predict(test_X)\n",
    "\n",
    "#     print(accuracy_score(Y_test, y_pred))\n",
    "#     #print(f1_score(Y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Zadatak1",
   "language": "python",
   "name": "zadatak1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
